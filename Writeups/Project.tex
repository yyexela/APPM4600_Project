\documentclass{article}

% Maths
\usepackage{amsmath}

% Hyperlink
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

% Figures
\usepackage{graphics, float, subfig}
\usepackage[pdflatex]{graphicx}

% Itemize
\renewcommand{\labelitemi}{\textbullet}
\renewcommand{\labelitemii}{\textbullet}
\renewcommand{\labelitemiii}{\textbullet}

% Margins
\usepackage[margin=1in]{geometry}

% Helpful commands
\newcommand{\x}{\mathbf{x}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{b}} % \b already defined
\newcommand{\I}{\mathbf{I}}
\newcommand{\D}{\mathbf{D}}

\begin{document}

% TODO: Make a nice title page
APPM 4600 Project 3: Regularization in Least Squares\\
Alexey Yermakov, Logan Barnhart, and Tyler Jensen

\section{Ridge Regression}
\subsection{Deriving the Ridge Estimator}

The equation for regularized least squares is:

\begin{equation} \label{eqn:rls}
    \arg \min_{x} ||\A\x-\B||_{2}^{2} + \gamma||\x||_{2}^{2}
\end{equation}

Recalling that $||\mathbf{x}||_{2}^{2}=\mathbf{x}^T \mathbf{x}$, we'll rewrite $||\mathbf{Ax}-\mathbf{b}||_{2}^{2} + \gamma||\mathbf{x}||_{2}^{2}$:

\begin{equation*}
\begin{split}
    & ||\A\x-\B||_{2}^{2} + \gamma||\x||_{2}^{2} =\\
    & =  (\A\x - \B)^T (\A\x - \B) + \gamma \x^T \x \\ 
    & =  ((\A\x)^T - \B^T) (\A\x - \B) + \gamma \x^T \x \\ 
    & =  (\x^T\A^T - \B^T) (\A\x - \B) + \gamma \x^T \x \\ 
    & = \x^T \A^T \A\x - \x^T \A^T \B - \B^T \A\x + \B^T \B + \gamma \x^T \x
\end{split}
\end{equation*}

Before we proceed further, we'll define what $\A$, $\B$, and $\x$ are. We want a least-squares fit to an $m$-degree polynomial $p_m(x)=a_0+a_1*x+\ldots+a_m*x^m$ where we have $n$ data points $\{x_0,b_0\}$. To have our system be overdetermined, we also assume $n > m$.

\begin{equation*}
    \A =
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^n & \ldots & x_n^m \\
    \end{bmatrix}
    \quad
    \x =
    \begin{bmatrix}
        a_0\\
        a_1\\
        \vdots\\
        a_m
    \end{bmatrix}
    \quad
    \B =
    \begin{bmatrix}
        b_0\\
        b_1\\
        \vdots\\
        b_n
    \end{bmatrix}
\end{equation*}

Where $dim(\A)=(n+1)\times(m+1)$, $dim(\x)=(m+1)\times(1)$, and $dim(\B)=(n+1)\times(1)$.

$ $ %Newline

We'll now prove $\x^T\A^T\B = \B^T\A\x$. Note first that $(\B^T\A\x)^T=\x^T\A^T\B$. Further, $dim(\x^T\A^T\B) = (1) \times (1) = dim(\B^T\A\x)$. Also, note that the transpose of a $1 \times 1$ matrix is the same matrix: $[c]^T=[c]$. It then follows that $\x^T\A^T\B = \B^T\A\x$, completing the proof.

$ $ %Newline

So,

\begin{equation} \label{eqn:rls_prod}
\begin{split}
    &\x^T \A^T \A\x - \x^T \A^T \B - \B^T \A\x + \B^T \B + \gamma \x^T \x = \\ 
    & = \x^T \A^T \A\x -2 \B^T \A\x + \B^T \B + \gamma \x^T \x\\
    & = (\A\x)^T(\A\x) -2 \B^T \A\x + \B^T \B + \gamma \x^T \x
\end{split}
\end{equation}

Now, we'll show what each of the above values (since each matrix is $1 \times 1$, meaning it's a scalar) actually is:

\begin{equation} \label{eqn:Ax}
\begin{split}
    & \A\x = \\
    & =
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^n & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        a_0\\
        a_1\\
        \vdots\\
        a_m
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
        a_0 + a_1 x_0 + a_2 x_0 ^2 + \ldots + a_m x_0^m \\
        a_0 + a_1 x_1 + a_2 x_1 ^2 + \ldots + a_m x_1^m \\
        a_0 + a_1 x_2 + a_2 x_2 ^2 + \ldots + a_m x_2^m \\
        \vdots \\
        a_0 + a_1 x_n + a_2 x_n ^2 + \ldots + a_m x_n^m \\
    \end{bmatrix}
\end{split}
\end{equation}

Then, $(\A\x)^T \A\x$ is just a simple inner product (we will use the result from \ref{eqn:Ax}):

\begin{equation} \label{eqn:axtax}
\begin{split}
    & (\A\x)^T \A\x = \\
    & =
    \begin{bmatrix}
        a_0 + a_1 x_0 + a_2 x_0 ^2 + \ldots + a_m x_0^m \\
        a_0 + a_1 x_1 + a_2 x_1 ^2 + \ldots + a_m x_1^m \\
        a_0 + a_1 x_2 + a_2 x_2 ^2 + \ldots + a_m x_2^m \\
        \vdots \\
        a_0 + a_1 x_n + a_2 x_n ^2 + \ldots + a_m x_n^m \\
    \end{bmatrix} ^T
    \times
    \begin{bmatrix}
        a_0 + a_1 x_0 + a_2 x_0 ^2 + \ldots + a_m x_0^m \\
        a_0 + a_1 x_1 + a_2 x_1 ^2 + \ldots + a_m x_1^m \\
        a_0 + a_1 x_2 + a_2 x_2 ^2 + \ldots + a_m x_2^m \\
        \vdots \\
        a_0 + a_1 x_n + a_2 x_n ^2 + \ldots + a_m x_n^m \\
    \end{bmatrix} \\
    & = (a_0 + a_1 x_0 + \ldots + a_m x_0^m)^ 2 + (a_0 + a_1 x_1 + \ldots + a_m x_1^m)^ 2\\
    & + \ldots + (a_0 + a_1 x_n + \ldots + a_m x_n^m)^ 2
\end{split}
\end{equation}

\begin{equation} \label{eqn:2bax}
\begin{split}
    & 2 \B ^T \A \x = \\
    & = 2 \times
    \begin{bmatrix}
        b_0 &
        b_1 &
        \ldots & 
        b_n
    \end{bmatrix}
    \times
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^2 & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        a_0\\
        a_1\\
        \vdots\\
        a_m
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        b_0 + b_1 + b_2 + \ldots + b_n \\
        b_0 x_0 + b_1 x_1 + b_2 x_2 + \ldots + b_n x_n \\
        b_0 x_0^2 + b_1 x_1^2 + b_2 x_2^2 + \ldots + b_n x_n^2 \\
        \vdots\\
        b_0 x_0^m + b_1 x_1^m + b_2 x_2^m + \ldots + b_n x_n^m \\
    \end{bmatrix} ^T
    \times
    \begin{bmatrix}
        a_0\\
        a_1\\
        \vdots\\
        a_m
    \end{bmatrix}\\
    & = 2(a_0 (b_0 + b_1 + \ldots + b_n) + a_1 (b_0 x_0 + b_1 x_1 + \ldots + b_n x_n) + \ldots + a_m (b_0 x_0^m + b_1 x_1^m + \ldots + b_n x_n^m))
\end{split}
\end{equation}

\begin{equation} \label{eqn:gxx}
\begin{split}
    & \gamma \x ^T \x = \\
    & \text{This is a simple inner product multiplied by a scalar} \\
    & = \gamma a_0 ^2 + \gamma a_1 ^2 + \ldots + \gamma a_m^2
\end{split}
\end{equation}

Great! Now lets take the derivates of \ref{eqn:axtax}, \ref{eqn:2bax}, and \ref{eqn:gxx} with respect to $\x$ to get the derivative of \ref{eqn:rls} with respect to $\x$. In effect, we'll get a vector where the $i$-th element is the derivative of \ref{eqn:rls} with respect to $a_i$.

First, let's take the derivatives of \ref{eqn:axtax}:

\begin{equation}
\begin{split}
    & \frac{d}{d\x} (\A\x)^T \A\x = \\
    & =
    \begin{bmatrix}
        \frac{d}{d a_0} (\A\x)^T \A\x \\
        \frac{d}{d a_1} (\A\x)^T \A\x \\
        \vdots \\
        \frac{d}{d a_m} (\A\x)^T \A\x \\
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
        2(a_0 + a_1 x_0 + \dots + a_m x_0 ^ m) + 2(a_0 + a_1 x_1 + \dots + a_m x_1 ^ m) + \ldots + 2(a_0 + a_1 x_n + \dots + a_m x_n ^ m)\\
        2 x_0(a_0 + a_1 x_0 + \dots + a_m x_0 ^ m) + 2 x_1(a_0 + a_1 x_1 + \dots + a_m x_1 ^ m) + \ldots + 2 x_n(a_0 + a_1 x_n + \dots + a_m x_n ^ m)\\
        \vdots \\
        2 x_0^m(a_0 + a_1 x_0 + \dots + a_m x_0 ^ m) + 2 x_1^m(a_0 + a_1 x_1 + \dots + a_m x_1 ^ m) + \ldots + 2 x_n^m(a_0 + a_1 x_n + \dots + a_m x_n ^ m)
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        a_0 \sum_{i=0}^{n} 1 + a_1 \sum_{i=0}^{n} x_i + a_2 \sum_{i=0}^{n} x_i^2 + \ldots + a_m \sum_{i=0}^{n} x_i ^ m\\
        a_0 \sum_{i=0}^{n} x_i + a_1 \sum_{i=0}^{n} x_i^2 + a_2 \sum_{i=0}^{n} x_i^3 + \ldots + a_m \sum_{i=0}^{n} x_i ^ {m+1}\\
        \vdots \\
        a_0 \sum_{i=0}^{n} x_i^m + a_1 \sum_{i=0}^{n} x_i^{m+1} + a_2 \sum_{i=0}^{n} x_i^{m+2} + \ldots + a_m \sum_{i=0}^{n} x_i ^ {2m}\\
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        \sum_{i=0}^{n} 1 & \sum_{i=0}^{n} x_i & \sum_{i=0}^{n} x_i ^2 & \ldots & \sum_{i=0}^{n} x_i ^m\\
        \sum_{i=0}^{n} x_i & \sum_{i=0}^{n} x_i ^2 & \sum_{i=0}^{n} x_i ^3 & \ldots & \sum_{i=0}^{n} x_i ^{m+1}\\
        \sum_{i=0}^{n} x_i ^2 & \sum_{i=0}^{n} x_i ^3 & \sum_{i=0}^{n} x_i ^4 & \ldots & \sum_{i=0}^{n} x_i ^{m+2}\\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        \sum_{i=0}^{n} x_i ^{m} & \sum_{i=0}^{n} x_i ^{m+1} & \sum_{i=0}^{n} x_i ^{m+2} & \ldots & \sum_{i=0}^{n} x_i ^{2m}
    \end{bmatrix}
    \times
    \begin{bmatrix}
        a_0 \\
        a_1 \\
        a_2 \\
        \vdots \\
        a_m
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        1 & 1 & 1 & \ldots & 1 \\
        x_0 & x_1 & x_2 & \ldots & x_n \\
        x_0^2 & x_1^2 & x_2 ^2 & \ldots & x_n^2 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_0^m & x_1^m & x_2 ^m & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^2 & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        a_0 \\
        a_1 \\
        a_2 \\
        \vdots \\
        a_m
    \end{bmatrix}\\
    &= 2\A ^T \A \x
\end{split}
\end{equation}

Secondly, let's take the derivatives of \ref{eqn:2bax}:

\begin{equation}
\begin{split}
    & \frac{d}{d\x} 2 \B ^T \A \x = \\
    & =
    \begin{bmatrix}
        \frac{d}{d a_0} 2 \B ^T \A \x \\
        \frac{d}{d a_1} 2 \B ^T \A \x \\
        \vdots \\
        \frac{d}{d a_m} 2 \B ^T \A \x \\
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        b_0 + b_1 + \ldots + b_n \\
        b_0x_0 + b_1x_1 + \ldots + b_nx_n \\
        b_0x_0^2 + b_1x_1^2 + \ldots + b_nx_n^2 \\
        \vdots \\
        b_0x_0^m + b_1x_1^m + \ldots + b_nx_n^m \\
    \end{bmatrix}\\
    &= 2 \times
    \begin{bmatrix}
        1 & 1 & 1 & \ldots & 1 \\
        x_0 & x_1 & x_2 & \ldots & x_n \\
        x_0^2 & x_1^2 & x_2 ^2 & \ldots & x_n^2 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_0^m & x_1^m & x_2 ^m & \ldots & x_n^m
    \end{bmatrix}
    \times
    \begin{bmatrix}
        b_0\\
        b_1\\
        \vdots\\
        b_n
    \end{bmatrix}\\
    &= 2 \A ^T \B
\end{split}
\end{equation}

Lastly, let's take the derivatives of \ref{eqn:gxx}:

\begin{equation}
\begin{split}
    & \frac{d}{d\x} \gamma \x^T \x = \\
    & =
    \begin{bmatrix}
        \frac{d}{d a_0} \gamma \x^T \x \\
        \frac{d}{d a_1} \gamma \x^T \x \\
        \vdots \\
        \frac{d}{d a_m} \gamma \x^T \x \\
    \end{bmatrix}\\
    &= \gamma \times
    \begin{bmatrix}
        2 a_0 \\
        2 a_1 \\
        2 a_2 \\
        \vdots \\
        2 a_m
    \end{bmatrix}\\
    &= 2 \times \gamma \times
    \begin{bmatrix}
        1 & 0 & 0 & \ldots & 0\\
        0 & 1 & 0 & \ldots & 0\\
        0 & 0 & 1 & \ldots & 0\\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \ldots & 1\\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        a_0 \\
        a_1 \\
        a_2 \\
        \vdots \\
        a_m
    \end{bmatrix}\\
    &= 2 \gamma \I \x
\end{split}
\end{equation}

Now we can recall \ref{eqn:rls_prod} and note that to find the minimum of the least squares equation given by \ref{eqn:rls} we can find where the derivative of \ref{eqn:rls_prod} is equal to zero:

\begin{equation}
\begin{split}
    \frac{d}{d \x} (\frac{}{}(\A\x)^T(\A\x) -2 \B^T \A\x + \B^T \B + \gamma \x^T \x) &= 0 \\
    2 \A ^T \A \x - 2 \A ^T \B + 2 \gamma \I \x &= 0 \\
    \A ^T \A \x - \A ^T \B + \gamma \I \x &= 0 \\
    \A ^T \A \x + \gamma \I \x - \A ^T \B &= 0 \\
    ( \A ^T \A + \gamma \I ) \x - \A ^T \B &= 0 \\
    ( \A ^T \A + \gamma \I ) \x  &= \A ^T \B \\
    \x &= ( \A ^T \A + \gamma \I )^{-1} \A ^T \B  \\
\end{split}
\end{equation}

We have now arrived at the equation for Ridge Regression! We note that there is a typo in the project description for the equation $E_{ridge}=( \A ^T \A + \gamma \I )^{-1} \A ^T$, where there is no trailing $\B$.

\section{Tikhonov Regression}
We can further generalize the loss function used for Ridge Regression if we notice that:


\begin{align*}
\arg \min_{x} ||\A\x-\B||_{2}^{2} + \gamma||\x||_{2}^{2} \\
\end{align*}


is equivalent to 


\begin{align*}
\arg \min_{x} ||\A\x-\B||_{2}^{2} + ||\sigma \I \x||_{2}^{2} \\
\end{align*}

where $\sigma = \sqrt{\gamma}$. We can generalize this by replacing $\sigma \I $ with various other \emph{weight matrices} to 'focus' the penalization on certain qualities or terms of $\x$. 

For example, we can use forward differences to estimate the derivative of a vector by using the matrix:

\begin{align*}
  \D = \begin{bmatrix}
    -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \dots & 0 \\
    0 & -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \vdots \\
    \vdots & 0 & \ddots & \ddots & \ddots & 0 \\
    0 & \dots & 0 & -\frac{1}{2} & 0 & \frac{1}{2} \\ 
  \end{bmatrix}\\
\end{align*}

Note that $\D$ is $(n-2) \times n $. If we put this in our loss function, it becomes: 

\begin{align*}
\arg \min_{x} ||\A\x-\B||_{2}^{2} + \lambda^2||\D \x||_{2}^{2} \\
\end{align*}

Where $\lambda$ is just a general weighting constant to control how much we want to penalize that last term. Now instead of just penalizing the magnitude of $\x$, we are penalizing its derivative - or in other words, forcing the resulting polynomial to be smooth. This will obviously change the solution of our estimator to solve the above loss function. 

\subsection{Exploring the Ridge Estimator}

With our derivation of the Ridge Estimator, we can now implement and test the Ridge Estimator on some data.
All code is in python 3, any random data was generated by using numpy's random number generator with seed 50. We will now construct
some data that we will test our Ridge Estimator implementation on. We take 20 random samples from the line $y = 3x + 2$ with Gaussian noise from a standard
normal distribtution. We now randomly sample 10 of these data points and will use them to fit/train our ridge regression model on, we will refer to these points as our 
training data, and the remaining 10 points we will call our validation data and will be used to measure the accuracy of our model using Residual Sum of Squares (also known as Sum of Squared Errors)

For this data we will try to find the line of best fit, that is find $m$ and $c$ in $y = mx + b$. We begin by trying $\gamma = 0$. This case reduces ridge regression to 
just normal linear regression/ordinary least squares. We also try $\gamma = 0.1$, this is the first real test of ridge regression. RSS compared below

From this we see that we achieved a lower RSS with $\gamma = 0.1$. That means that we have already improved over ordinary least squares! Ridge Regression has already shown to be an improvement. 
But we just tried some random $\gamma$ value, are there other values of $\gamma$ where the performance is even better? There's no analytic way to test this,
so we try a whole lot of $\gamma$'s and see what happens.


From this graph we see that our lowest RSS's occur for $\gamma$'s from $[0,10]$. We now try those $\gamma$'s. 

We achieve a min RSS of at $\gamma = 1$. Thus ridge regression gives us a model that performs about % 
better than what we had with ordinary least square. That's quite an exciting result. Let's see if this translates to other functions/models.


We now consider $y = x^2$. We begin by taking 20 random samples from the interval $[-5,5]$ and then splitting the data into our training data and validation
data as we did for $y = 3x + 2$. 

Now we want to try and fit our data to the model $y = $. We try a similar approach as we did before, testing 
\subsection{Deriving the Tikhonov Estimator}

As we saw when deriving the ridge estimator, to find this estimator we want to solve

\begin{align*}
\frac{d}{d\x} \left[ ||\A\x-\B||_{2}^{2} + \lambda^2||\D \x||_{2}^{2} \right] = 0 \\
\end{align*}
or, if we expand that similar to the loss function for ridge estimation, 

\begin{align*}
\frac{d}{d\x} \left[ (\A\x)^T(\A\x) -2 \B^T \A\x + \B^T \B + \lambda^2(\D\x)^{\textrm{T}}(\D\x) \right] = 0 \\
\end{align*}

From the ridge estimator derivation we already know that 
\begin{align*}
\frac{d}{d\x} [ (\A\x)^{\textrm{T}} \A\x ] = 2\A^{\textrm{T}}\A\x \\
\frac{d}{d\x} [ (2\B^{\textrm{T}} \A\x ] = 2\A^{\textrm{T}}\B
\end{align*}

So what is $\frac{d}{d\x} [ (\D\x)^{\textrm{T}}\D\x ] $? Well, 

\begin{align*}
  \D\x &= \begin{bmatrix}
    -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \dots & 0 \\
    0 & -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \vdots \\
    \vdots & 0 & \ddots & \ddots & \ddots & 0 \\
    0 & \dots & 0 & -\frac{1}{2} & 0 & \frac{1}{2} \\ 
  \end{bmatrix} 
  \begin{bmatrix}
  a_0 \\
  a_1 \\
  \vdots \\
  a_n
  \end{bmatrix} \\\\
  &= \frac{1}{2}\begin{bmatrix}
  a_2 - a_0 \\ 
  a_3 - a_1 \\ 
  a_4 - a_2 \\
  \vdots \\
  a_n - a_{n-2}
  \end{bmatrix}
\end{align*}

So it follows that

\begin{align*}
  &(\D\x)^{\textrm{T}}(\D\x) = 
  \begin{bmatrix}
  \frac{a_2 - a_0}{2} \\ 
  \frac{a_3 - a_1}{2} \\ 
  \frac{a_4 - a_2}{2} \\
  \vdots \\
  \frac{a_n - a_{n-2}}{2}
  \end{bmatrix} ^ {\textrm{T}}
   \begin{bmatrix}
  \frac{a_2 - a_0}{2} \\ 
  \frac{a_3 - a_1}{2} \\ 
  \frac{a_4 - a_2}{2} \\
  \vdots \\
  \frac{a_n - a_{n-2}}{2}
  \end{bmatrix} \\\\
  &= \left( \frac{(a_2 - a_0)^2}{4} + \frac{(a_3 - a_1)^2}{4} + \dots \frac{(a_n - a_{n-2})^2}{4} \right)
\end{align*}

Then it follows that 

\begin{align*}
\frac{d}{d\x} \left[ (\D\x)^{\textrm{T}}(\D\x) \right] = 
\begin{bmatrix}
- \frac{a_2 - a_0}{2} \\\\
- \frac{a_3 - a_2}{2} \\\\
\frac{a_2 - a_0}{2} - \frac{a_4 - a_2}{2}\\
\vdots \\
\frac{a_{n-2} - a_{n-4}}{2} - \frac{a_n - a_{n-2}}{2}\\\\
\frac{a_{n-1} - a_{n-3}}{2} \\\\
\frac{a_n - a_n-2}{2}
\end{bmatrix}
\end{align*}

This initially doesn't seem like anything useful, but note that

\begin{align*}
\D^{\textrm{T}}\D\x &= 
\begin{bmatrix}
  -\frac{1}{2} & 0  & 0 & \dots & 0\\
  0 & -\frac{1}{2} & 0  & \dots & 0 \\
  \frac{1}{2} & 0 & \ddots   & \dots & 0 \\
  0 & \frac{1}{2} & 0  & -\frac{1}{2}  & 0\\
  \vdots & 0 & \ddots & 0& -\frac{1}{2} \\
  0 & \dots & 0 & \frac{1}{2} & 0 \\
  0 & \dots & 0 & 0 & \frac{1}{2}
\end{bmatrix}
\begin{bmatrix}
\frac{a_2 - a_0}{2} \\
\frac{a_3 - a_1}{2} \\
\vdots \\
\frac{a_n - a_{n-2}}{2} \\
\end{bmatrix} \\ \\
&= 
\begin{bmatrix}
-\frac{a_2 - a_0}{4} \\\\
-\frac{a_3 - a_1}{4} \\\\
\frac{a_2 - a_0}{4} - \frac{a_4 - a_2}{4} \\\\
\vdots \\
\frac{a_{n-2} - a_{n-4}}{4} - \frac{a_n - a_{n-2}}{4}\\\\
\frac{a_{n-1} - a_{n-3}}{4} \\\\
\frac{a_n - a_n-2}{4}
\end{bmatrix}
\end{align*}
So, it's true that 
\begin{align*}
\frac{d}{d\x} \left[ (\D\x)^{\textrm{T}}(\D\x) \right] = 2\D^{\textrm{T}}\D\x\\
\end{align*}

Finally we can solve for the $\x$ that satisfies

\begin{align*}
\frac{d}{d\x} \left[||\A\x-\B||_{2}^{2} + \lambda^2||\D \x||_{2}^{2} \right] = 0 \\
\end{align*}
or,
\begin{align*}
 2\A^{\textrm{T}}\A\x - 2\A^{\textrm{T}}\B + 2\lambda^2 \D^{\textrm{T}}\D\x = 0 \\
 \A^{\textrm{T}}\A\x - \A^{\textrm{T}}\B + \lambda^2 \D^{\textrm{T}}\D\x = 0 \\
 \A^{\textrm{T}}\A\x  + \lambda^2 \D^{\textrm{T}}\D\x = \A^{\textrm{T}}\B\\
  (\A^{\textrm{T}}\A  + \lambda^2 \D^{\textrm{T}}\D)\x = \A^{\textrm{T}}\B\\
\end{align*}
Thus
\begin{align*}
\x = (\A^{\textrm{T}}\A  + \lambda^2 \D^{\textrm{T}}\D)^{-1}\A^{\textrm{T}}\B\\
\end{align*}

Generally speaking, a loss function with weight matrix $\mathbf{\Gamma}$ of the form

\begin{align*}
\arg \min_{x}||\A\x-\B||_{2}^{2} + ||\textbf{$\Gamma$} \x||_{2}^{2}
\end{align*}

will be solved by 

\begin{align*}
\x = (\A^{\textrm{T}}\A  + \mathbf{\Gamma}^{\textrm{T}}\mathbf{\Gamma})^{-1}\A^{\textrm{T}}\B\\
\end{align*}

but weight matrices can be customized to such a degree that it's best to verify this property for each case. 

\end{document}