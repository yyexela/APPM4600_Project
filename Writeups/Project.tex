\documentclass{article}

% Maths
\usepackage{amsmath}

% Hyperlink
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

% Figures
\usepackage{graphics, float, subfig}
\usepackage[pdflatex]{graphicx}

% Itemize
\renewcommand{\labelitemi}{\textbullet}
\renewcommand{\labelitemii}{\textbullet}
\renewcommand{\labelitemiii}{\textbullet}

% Margins
\usepackage[margin=1in]{geometry}

% Helpful commands
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\boldsymbol\beta} % \b already defined
\newcommand{\I}{\mathbf{I}}
\newcommand{\D}{\mathbf{D}}

\begin{document}

% TODO: Make a nice title page
APPM 4600 Project 3: Regularization in Least Squares\\
Alexey Yermakov, Logan Barnhart, and Tyler Jensen

\section{Ridge Regression}
\subsection{Deriving the Ridge Estimator}

The equation for regularized least squares is:

\begin{equation} \label{eqn:rls}
    \arg \min_{\B} ||\X\B-\y||_{2}^{2} + \gamma||\B||_{2}^{2}
\end{equation}

Recalling that $||\B||_{2}^{2}=\B^T \B$, we'll rewrite $||\X\B-\y||_{2}^{2} + \gamma||\B||_{2}^{2}$:

\begin{equation*}
\begin{split}
    & ||\X\B-\y||_{2}^{2} + \gamma||\B||_{2}^{2} =\\
    & =  (\X\B - \y)^T (\X\B - \y) + \gamma \B^T \B \\ 
    & =  ((\X\B)^T - \y^T) (\X\B - \y) + \gamma \B^T \B \\ 
    & =  (\B^T\X^T - \y^T) (\X\B - \y) + \gamma \B^T \B \\ 
    & = \B^T \X^T \X\B - \B^T \X^T \y - \y^T \X\B + \y^T \y + \gamma \B^T \B
\end{split}
\end{equation*}

Before we proceed further, we'll define what $\X$, $\y$, and $\B$ are. We want a least-squares fit to an $m$-degree polynomial $p_m(x)=\beta_0+\beta_1*x+\ldots+\beta_m*x^m$ where we have $n$ data points $\{x_0,y_0\}$. To have our system be overdetermined, we also assume $n > m$.

\begin{equation*}
    \X =
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^n & \ldots & x_n^m \\
    \end{bmatrix}
    \quad
    \B =
    \begin{bmatrix}
        \beta_0\\
        \beta_1\\
        \vdots\\
        \beta_m
    \end{bmatrix}
    \quad
    \y =
    \begin{bmatrix}
        y_0\\
        y_1\\
        \vdots\\
        y_n
    \end{bmatrix}
\end{equation*}

Where $dim(\X)=(n+1)\times(m+1)$, $dim(\B)=(m+1)\times(1)$, and $dim(\y)=(n+1)\times(1)$.

$ $ %Newline

We'll now prove $\B^T\X^T\y = \y^T\X\B$. Note first that $(\y^T\X\B)^T=\B^T\X^T\y$. Further, $dim(\B^T\X^T\y) = (1) \times (1) = dim(\y^T\X\B)$. Also, note that the transpose of a $1 \times 1$ matrix is the same matrix: $[c]^T=[c]$. It then follows that $\B^T\X^T\y = \y^T\X\B$, completing the proof.

$ $ %Newline

So,

\begin{equation} \label{eqn:rls_prod}
\begin{split}
    &\B^T \X^T \X\B - \B^T \X^T \y - \y^T \X\B + \y^T \y + \gamma \B^T \B = \\ 
    & = \B^T \X^T \X\B -2 \y^T \X\B + \y^T \y + \gamma \B^T \B\\
    & = (\X\B)^T(\X\B) -2 \y^T \X\B + \y^T \y + \gamma \B^T \B
\end{split}
\end{equation}

Now, we'll show what each of the above values (since each matrix is $1 \times 1$, meaning it's a scalar) actually is:

\begin{equation} \label{eqn:Ax}
\begin{split}
    & \X\B = \\
    & =
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^n & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        \beta_0\\
        \beta_1\\
        \vdots\\
        \beta_m
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
        \beta_0 + \beta_1 x_0 + \beta_2 x_0 ^2 + \ldots + \beta_m x_0^m \\
        \beta_0 + \beta_1 x_1 + \beta_2 x_1 ^2 + \ldots + \beta_m x_1^m \\
        \beta_0 + \beta_1 x_2 + \beta_2 x_2 ^2 + \ldots + \beta_m x_2^m \\
        \vdots \\
        \beta_0 + \beta_1 x_n + \beta_2 x_n ^2 + \ldots + \beta_m x_n^m \\
    \end{bmatrix}
\end{split}
\end{equation}

Then, $(\X\B)^T \X\B$ is just a simple inner product (we will use the result from \ref{eqn:Ax}):

\begin{equation} \label{eqn:axtax}
\begin{split}
    & (\X\B)^T \X\B = \\
    & =
    \begin{bmatrix}
        \beta_0 + \beta_1 x_0 + \beta_2 x_0 ^2 + \ldots + \beta_m x_0^m \\
        \beta_0 + \beta_1 x_1 + \beta_2 x_1 ^2 + \ldots + \beta_m x_1^m \\
        \beta_0 + \beta_1 x_2 + \beta_2 x_2 ^2 + \ldots + \beta_m x_2^m \\
        \vdots \\
        \beta_0 + \beta_1 x_n + \beta_2 x_n ^2 + \ldots + \beta_m x_n^m \\
    \end{bmatrix} ^T
    \times
    \begin{bmatrix}
        \beta_0 + \beta_1 x_0 + \beta_2 x_0 ^2 + \ldots + \beta_m x_0^m \\
        \beta_0 + \beta_1 x_1 + \beta_2 x_1 ^2 + \ldots + \beta_m x_1^m \\
        \beta_0 + \beta_1 x_2 + \beta_2 x_2 ^2 + \ldots + \beta_m x_2^m \\
        \vdots \\
        \beta_0 + \beta_1 x_n + \beta_2 x_n ^2 + \ldots + \beta_m x_n^m \\
    \end{bmatrix} \\
    & = (\beta_0 + \beta_1 x_0 + \ldots + \beta_m x_0^m)^ 2 + (\beta_0 + \beta_1 x_1 + \ldots + \beta_m x_1^m)^ 2\\
    & + \ldots + (\beta_0 + \beta_1 x_n + \ldots + \beta_m x_n^m)^ 2
\end{split}
\end{equation}

\begin{equation} \label{eqn:2bax}
\begin{split}
    & 2 \y ^T \X \B = \\
    & = 2 \times
    \begin{bmatrix}
        y_0 &
        y_1 &
        \ldots & 
        y_n
    \end{bmatrix}
    \times
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^2 & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        \beta_0\\
        \beta_1\\
        \vdots\\
        \beta_m
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        y_0 + y_1 + y_2 + \ldots + y_n \\
        y_0 x_0 + y_1 x_1 + y_2 x_2 + \ldots + y_n x_n \\
        y_0 x_0^2 + y_1 x_1^2 + y_2 x_2^2 + \ldots + y_n x_n^2 \\
        \vdots\\
        y_0 x_0^m + y_1 x_1^m + y_2 x_2^m + \ldots + y_n x_n^m \\
    \end{bmatrix} ^T
    \times
    \begin{bmatrix}
        \beta_0\\
        \beta_1\\
        \vdots\\
        \beta_m
    \end{bmatrix}\\
    & = 2(\beta_0 (y_0 + y_1 + \ldots + y_n) + \beta_1 (y_0 x_0 + y_1 x_1 + \ldots + y_n x_n) + \ldots + \beta_m (y_0 x_0^m + y_1 x_1^m + \ldots + y_n x_n^m))
\end{split}
\end{equation}

\begin{equation} \label{eqn:gxx}
\begin{split}
    & \gamma \B ^T \B = \\
    & \text{This is a simple inner product multiplied by a scalar} \\
    & = \gamma \beta_0 ^2 + \gamma \beta_1 ^2 + \ldots + \gamma \beta_m^2
\end{split}
\end{equation}

Great! Now lets take the derivates of \ref{eqn:axtax}, \ref{eqn:2bax}, and \ref{eqn:gxx} with respect to $\B$ to get the derivative of \ref{eqn:rls} with respect to $\B$. In effect, we'll get a vector where the $i$-th element is the derivative of \ref{eqn:rls} with respect to $\beta_i$.

First, let's take the derivatives of \ref{eqn:axtax}:

\begin{equation}
\begin{split}
    & \frac{d}{d\B} (\X\B)^T \X\B = \\
    & =
    \begin{bmatrix}
        \frac{d}{d \beta_0} (\X\B)^T \X\B \\
        \frac{d}{d \beta_1} (\X\B)^T \X\B \\
        \vdots \\
        \frac{d}{d \beta_m} (\X\B)^T \X\B \\
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
        2(\beta_0 + \beta_1 x_0 + \dots + \beta_m x_0 ^ m) + 2(\beta_0 + \beta_1 x_1 + \dots + \beta_m x_1 ^ m) + \ldots + 2(\beta_0 + \beta_1 x_n + \dots + \beta_m x_n ^ m)\\
        2 x_0(\beta_0 + \beta_1 x_0 + \dots + \beta_m x_0 ^ m) + 2 x_1(\beta_0 + \beta_1 x_1 + \dots + \beta_m x_1 ^ m) + \ldots + 2 x_n(\beta_0 + \beta_1 x_n + \dots + \beta_m x_n ^ m)\\
        \vdots \\
        2 x_0^m(\beta_0 + \beta_1 x_0 + \dots + \beta_m x_0 ^ m) + 2 x_1^m(\beta_0 + \beta_1 x_1 + \dots + \beta_m x_1 ^ m) + \ldots + 2 x_n^m(\beta_0 + \beta_1 x_n + \dots + \beta_m x_n ^ m)
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        \beta_0 \sum_{i=0}^{n} 1 + \beta_1 \sum_{i=0}^{n} x_i + \beta_2 \sum_{i=0}^{n} x_i^2 + \ldots + \beta_m \sum_{i=0}^{n} x_i ^ m\\
        \beta_0 \sum_{i=0}^{n} x_i + \beta_1 \sum_{i=0}^{n} x_i^2 + \beta_2 \sum_{i=0}^{n} x_i^3 + \ldots + \beta_m \sum_{i=0}^{n} x_i ^ {m+1}\\
        \vdots \\
        \beta_0 \sum_{i=0}^{n} x_i^m + \beta_1 \sum_{i=0}^{n} x_i^{m+1} + \beta_2 \sum_{i=0}^{n} x_i^{m+2} + \ldots + \beta_m \sum_{i=0}^{n} x_i ^ {2m}\\
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        \sum_{i=0}^{n} 1 & \sum_{i=0}^{n} x_i & \sum_{i=0}^{n} x_i ^2 & \ldots & \sum_{i=0}^{n} x_i ^m\\
        \sum_{i=0}^{n} x_i & \sum_{i=0}^{n} x_i ^2 & \sum_{i=0}^{n} x_i ^3 & \ldots & \sum_{i=0}^{n} x_i ^{m+1}\\
        \sum_{i=0}^{n} x_i ^2 & \sum_{i=0}^{n} x_i ^3 & \sum_{i=0}^{n} x_i ^4 & \ldots & \sum_{i=0}^{n} x_i ^{m+2}\\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        \sum_{i=0}^{n} x_i ^{m} & \sum_{i=0}^{n} x_i ^{m+1} & \sum_{i=0}^{n} x_i ^{m+2} & \ldots & \sum_{i=0}^{n} x_i ^{2m}
    \end{bmatrix}
    \times
    \begin{bmatrix}
        \beta_0 \\
        \beta_1 \\
        \beta_2 \\
        \vdots \\
        \beta_m
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        1 & 1 & 1 & \ldots & 1 \\
        x_0 & x_1 & x_2 & \ldots & x_n \\
        x_0^2 & x_1^2 & x_2 ^2 & \ldots & x_n^2 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_0^m & x_1^m & x_2 ^m & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        1 & x_0 & x_0 ^2 & \ldots & x_0^m \\
        1 & x_1 & x_1 ^2 & \ldots & x_1^m \\
        1 & x_2 & x_2 ^2 & \ldots & x_2^m \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_n & x_n ^2 & \ldots & x_n^m \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        \beta_0 \\
        \beta_1 \\
        \beta_2 \\
        \vdots \\
        \beta_m
    \end{bmatrix}\\
    &= 2\X ^T \X \B
\end{split}
\end{equation}

Secondly, let's take the derivatives of \ref{eqn:2bax}:

\begin{equation}
\begin{split}
    & \frac{d}{d\B} 2 \y ^T \X \B = \\
    & =
    \begin{bmatrix}
        \frac{d}{d a_0} 2 \y ^T \X \B \\
        \frac{d}{d a_1} 2 \y ^T \X \B \\
        \vdots \\
        \frac{d}{d a_m} 2 \y ^T \X \B \\
    \end{bmatrix}\\
    & = 2 \times
    \begin{bmatrix}
        y_0 + y_1 + \ldots + y_n \\
        y_0x_0 + y_1x_1 + \ldots + y_nx_n \\
        y_0x_0^2 + y_1x_1^2 + \ldots + y_nx_n^2 \\
        \vdots \\
        y_0x_0^m + y_1x_1^m + \ldots + y_nx_n^m \\
    \end{bmatrix}\\
    &= 2 \times
    \begin{bmatrix}
        1 & 1 & 1 & \ldots & 1 \\
        x_0 & x_1 & x_2 & \ldots & x_n \\
        x_0^2 & x_1^2 & x_2 ^2 & \ldots & x_n^2 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_0^m & x_1^m & x_2 ^m & \ldots & x_n^m
    \end{bmatrix}
    \times
    \begin{bmatrix}
        y_0\\
        y_1\\
        \vdots\\
        y_n
    \end{bmatrix}\\
    &= 2 \X ^T \y
\end{split}
\end{equation}

Lastly, let's take the derivatives of \ref{eqn:gxx}:

\begin{equation}
\begin{split}
    & \frac{d}{d\B} \gamma \B^T \B = \\
    & =
    \begin{bmatrix}
        \frac{d}{d \beta_0} \gamma \B^T \B \\
        \frac{d}{d \beta_1} \gamma \B^T \B \\
        \vdots \\
        \frac{d}{d \beta_m} \gamma \B^T \B \\
    \end{bmatrix}\\
    &= \gamma \times
    \begin{bmatrix}
        2 \beta_0 \\
        2 \beta_1 \\
        2 \beta_2 \\
        \vdots \\
        2 \beta_m
    \end{bmatrix}\\
    &= 2 \times \gamma \times
    \begin{bmatrix}
        1 & 0 & 0 & \ldots & 0\\
        0 & 1 & 0 & \ldots & 0\\
        0 & 0 & 1 & \ldots & 0\\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \ldots & 1\\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        \beta_0 \\
        \beta_1 \\
        \beta_2 \\
        \vdots \\
        \beta_m
    \end{bmatrix}\\
    &= 2 \gamma \I \B
\end{split}
\end{equation}

Now we can recall \ref{eqn:rls_prod} and note that to find the minimum of the least squares equation given by \ref{eqn:rls} we can find where the derivative of \ref{eqn:rls_prod} is equal to zero:

\begin{equation}
\begin{split}
    \frac{d}{d \B} (\frac{}{}(\X\B)^T(\X\B) -2 \y^T \X\B + \y^T \y + \gamma \B^T \B) &= 0 \\
    2 \X ^T \X \B - 2 \X ^T \y + 2 \gamma \I \B &= 0 \\
    \X ^T \X \B - \X ^T \y + \gamma \I \B &= 0 \\
    \X ^T \X \B + \gamma \I \B - \X ^T \y &= 0 \\
    ( \X ^T \X + \gamma \I ) \B - \X ^T \y &= 0 \\
    ( \X ^T \X + \gamma \I ) \B  &= \X ^T \y \\
    \B &= ( \X ^T \X + \gamma \I )^{-1} \X ^T \y  \\
\end{split}
\end{equation}

We have now arrived at the equation for Ridge Regression! We note that there is a typo in the project description for the equation $E_{ridge}=( \X ^T \X + \gamma \I )^{-1} \X ^T$, where there is no trailing $\y$.

\section{Tikhonov Regression}
We can further generalize the loss function used for Ridge Regression if we notice that:


\begin{align*}
\arg \min_{x} ||\X\B-\y||_{2}^{2} + \gamma||\B||_{2}^{2} \\
\end{align*}


is equivalent to 


\begin{align*}
\arg \min_{x} ||\X\B-\y||_{2}^{2} + ||\sigma \I \B||_{2}^{2} \\
\end{align*}

where $\sigma = \sqrt{\gamma}$. We can generalize this by replacing $\sigma \I $ with various other \emph{weight matrices} to 'focus' the penalization on certain qualities or terms of $\B$. 

For example, we can use forward differences to estimate the derivative of a vector by using the matrix:

\begin{align*}
  \D = \begin{bmatrix}
    -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \dots & 0 \\
    0 & -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \vdots \\
    \vdots & 0 & \ddots & \ddots & \ddots & 0 \\
    0 & \dots & 0 & -\frac{1}{2} & 0 & \frac{1}{2} \\ 
  \end{bmatrix}\\
\end{align*}

Note that $\D$ is $(n-2) \times n $. If we put this in our loss function, it becomes: 

\begin{align*}
\arg \min_{x} ||\X\B-\y||_{2}^{2} + \lambda^2||\D \B||_{2}^{2} \\
\end{align*}

Where $\lambda$ is just a general weighting constant to control how much we want to penalize that last term. Now instead of just penalizing the magnitude of $\B$, we are penalizing its derivative - or in other words, forcing the resulting polynomial to be smooth. This will obviously change the solution of our estimator to solve the above loss function. 


\subsection{Deriving the Tikhonov Estimator}

As we saw when deriving the ridge estimator, to find this estimator we want to solve

\begin{align*}
\frac{d}{d\B} \left[ ||\X\B-\y||_{2}^{2} + \lambda^2||\D \B||_{2}^{2} \right] = 0 \\
\end{align*}
or, if we expand that similar to the loss function for ridge estimation, 

\begin{align*}
\frac{d}{d\B} \left[ (\X\B)^T(\X\B) -2 \y^T \X\B + \y^T \y + \lambda^2(\D\B)^{\textrm{T}}(\D\B) \right] = 0 \\
\end{align*}

From the ridge estimator derivation we already know that 
\begin{align*}
\frac{d}{d\B} [ (\X\B)^{\textrm{T}} \X\B ] = 2\X^{\textrm{T}}\X\B \\
\frac{d}{d\B} [ (2\y^{\textrm{T}} \X\B ] = 2\X^{\textrm{T}}\y
\end{align*}

So what is $\frac{d}{d\B} [ (\D\B)^{\textrm{T}}\D\B ] $? Well, 

\begin{align*}
  \D\B &= \begin{bmatrix}
    -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \dots & 0 \\
    0 & -\frac{1}{2} & 0 & \frac{1}{2} & 0 & \vdots \\
    \vdots & 0 & \ddots & \ddots & \ddots & 0 \\
    0 & \dots & 0 & -\frac{1}{2} & 0 & \frac{1}{2} \\ 
  \end{bmatrix} 
  \begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
  \vdots \\
  \beta_n
  \end{bmatrix} \\\\
  &= \frac{1}{2}\begin{bmatrix}
  \beta_2 - \beta_0 \\ 
  \beta_3 - \beta_1 \\ 
  \beta_4 - \beta_2 \\
  \vdots \\
  \beta_n - \beta_{n-2}
  \end{bmatrix}
\end{align*}

So it follows that

\begin{align*}
  &(\D\B)^{\textrm{T}}(\D\B) = 
  \begin{bmatrix}
  \frac{\beta_2 - \beta_0}{2} \\ 
  \frac{\beta_3 - \beta_1}{2} \\ 
  \frac{\beta_4 - \beta_2}{2} \\
  \vdots \\
  \frac{\beta_n - \beta_{n-2}}{2}
  \end{bmatrix} ^ {\textrm{T}}
   \begin{bmatrix}
  \frac{\beta_2 - \beta_0}{2} \\ 
  \frac{\beta_3 - \beta_1}{2} \\ 
  \frac{\beta_4 - \beta_2}{2} \\
  \vdots \\
  \frac{\beta_n - \beta_{n-2}}{2}
  \end{bmatrix} \\\\
  &= \left( \frac{(\beta_2 - \beta_0)^2}{4} + \frac{(\beta_3 - \beta_1)^2}{4} + \dots \frac{(\beta_n - \beta_{n-2})^2}{4} \right)
\end{align*}

Then it follows that 

\begin{align*}
\frac{d}{d\B} \left[ (\D\B)^{\textrm{T}}(\D\B) \right] = 
\begin{bmatrix}
- \frac{\beta_2 - \beta_0}{2} \\\\
- \frac{\beta_3 - \beta_2}{2} \\\\
\frac{\beta_2 - \beta_0}{2} - \frac{\beta_4 - \beta_2}{2}\\
\vdots \\
\frac{\beta_{n-2} - \beta_{n-4}}{2} - \frac{\beta_n - \beta_{n-2}}{2}\\\\
\frac{\beta_{n-1} - \beta_{n-3}}{2} \\\\
\frac{\beta_n - \beta_n-2}{2}
\end{bmatrix}
\end{align*}

This initially doesn't seem like anything useful, but note that

\begin{align*}
\D^{\textrm{T}}\D\B &= 
\begin{bmatrix}
  -\frac{1}{2} & 0  & 0 & \dots & 0\\
  0 & -\frac{1}{2} & 0  & \dots & 0 \\
  \frac{1}{2} & 0 & \ddots   & \dots & 0 \\
  0 & \frac{1}{2} & 0  & -\frac{1}{2}  & 0\\
  \vdots & 0 & \ddots & 0& -\frac{1}{2} \\
  0 & \dots & 0 & \frac{1}{2} & 0 \\
  0 & \dots & 0 & 0 & \frac{1}{2}
\end{bmatrix}
\begin{bmatrix}
\frac{\beta_2 - \beta_0}{2} \\
\frac{\beta_3 - \beta_1}{2} \\
\vdots \\
\frac{\beta_n - \beta_{n-2}}{2} \\
\end{bmatrix} \\ \\
&= 
\begin{bmatrix}
-\frac{\beta_2 - \beta_0}{4} \\\\
-\frac{\beta_3 - \beta_1}{4} \\\\
\frac{\beta_2 - \beta_0}{4} - \frac{\beta_4 - \beta_2}{4} \\\\
\vdots \\
\frac{\beta_{n-2} - \beta_{n-4}}{4} - \frac{\beta_n - \beta_{n-2}}{4}\\\\
\frac{\beta_{n-1} - \beta_{n-3}}{4} \\\\
\frac{\beta_n - \beta_n-2}{4}
\end{bmatrix}
\end{align*}
So, it's true that 
\begin{align*}
\frac{d}{d\B} \left[ (\D\B)^{\textrm{T}}(\D\B) \right] = 2\D^{\textrm{T}}\D\B\\
\end{align*}

Finally we can solve for the $\B$ that satisfies

\begin{align*}
\frac{d}{d\B} \left[||\X\B-\y||_{2}^{2} + \lambda^2||\D \B||_{2}^{2} \right] = 0 \\
\end{align*}
or,
\begin{align*}
 2\X^{\textrm{T}}\X\B - 2\X^{\textrm{T}}\y + 2\lambda^2 \D^{\textrm{T}}\D\B = 0 \\
 \X^{\textrm{T}}\X\B - \X^{\textrm{T}}\y + \lambda^2 \D^{\textrm{T}}\D\B = 0 \\
 \X^{\textrm{T}}\X\B  + \lambda^2 \D^{\textrm{T}}\D\B = \X^{\textrm{T}}\y\\
  (\X^{\textrm{T}}\X  + \lambda^2 \D^{\textrm{T}}\D)\B = \X^{\textrm{T}}\y\\
\end{align*}
Thus
\begin{align*}
\B = (\X^{\textrm{T}}\X  + \lambda^2 \D^{\textrm{T}}\D)^{-1}\X^{\textrm{T}}\y\\
\end{align*}

Generally speaking, a loss function with weight matrix $\mathbf{\Gamma}$ of the form

\begin{align*}
\arg \min_{x}||\X\B-\y||_{2}^{2} + ||\textbf{$\Gamma$} \B||_{2}^{2}
\end{align*}

will be solved by 

\begin{align*}
\B = (\X^{\textrm{T}}\X  + \mathbf{\Gamma}^{\textrm{T}}\mathbf{\Gamma})^{-1}\X^{\textrm{T}}\y\\
\end{align*}

but weight matrices can be customized to such a degree that it's best to verify this property for each case. 

\end{document}